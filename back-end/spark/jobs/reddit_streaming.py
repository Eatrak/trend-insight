import os
import re
import time
import requests
from pyspark.sql import SparkSession, functions as F

# Kafka Bootstrap Servers: The address of the Kafka cluster.
KAFKA_URL         = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "kafka:9092")

# API Endpoint: The internal API to fetch topics from.
# In production, this should be an internal DNS/Service Discovery name.
API_TOPICS_URL    = os.getenv("API_TOPICS_URL", "http://trend-api:8000/topics")

# Checkpoint Directory: Critical for Structured Streaming fault tolerance.
CHECKPOINT_DIR    = "/checkpoints"

# Output Topic: Where we send the calculated metrics.
DAILY_METRICS_OUT = "reddit.topic.metrics"

# Input Schema: Defines the structure of the JSON data coming from Reddit.
POST_SCHEMA       = "event_id STRING, created_utc STRING, text STRING, score INT, num_comments INT"


# Global cache for workers. Spark recycles Python worker processes, so this global
# persists across multiple tasks/micro-batches processed by the same worker.
_TOPIC_CACHE = {"regex": None, "topics": [], "last_updated": 0}

def get_topics_cached():
    """
    Worker-side caching mechanism.
    
    This function ensures that the topic definitions (keywords) are loaded
    into memory. It fetches from the API at most once every 60 seconds.
    """
    now = time.time()
    
    # 1. Check refresh interval
    if now - _TOPIC_CACHE["last_updated"] > 10:
        try:
            # 2. Fetch from API
            # Timeout is critical. If API is slow, don't block the stream forever.
            response = requests.get(API_TOPICS_URL, timeout=5)
            
            if response.status_code == 200:
                topics_list = response.json()
                
                parsed_topics = []
                all_terms = set()
                
                for r in topics_list:
                    # Skip inactive topics if the API returns them
                    if not r.get("is_active", True):
                        continue
                        
                    raw_keywords = r.get("keywords")
                    
                    # Normalized Structure: List of Lists of Strings (CNF)
                    # [[A, B], [C]] -> (A OR B) AND (C)
                    groups = []
                    
                    if isinstance(raw_keywords, list):
                        if not raw_keywords: continue # Empty list
                        
                        # Strict New Structure: List[List[str]] i.e. CNF
                        # We assume the API strictly returns list of lists.
                        groups = []
                        for g in raw_keywords:
                            if isinstance(g, list):
                                groups.append([str(x).strip().lower() for x in g])
                    
                    if not groups: continue

                    # Collect terms for global regex
                    for g in groups:
                        for term in g:
                            all_terms.add(term)
                            
                    parsed_topics.append({
                        "id": r["id"],
                        "groups": groups
                    })
                
                # 3. Compile Regex
                if all_terms:
                    # Sort by length descending. This is CRITICAL for regex correctness.
                    sorted_terms = sorted(list(all_terms), key=len, reverse=True)
                    
                    # Create one giant pattern: (term1|term2|term3)
                    pattern = "|".join([re.escape(t) for t in sorted_terms])
                    _TOPIC_CACHE["regex"] = re.compile(f"(?i)({pattern})")
                else:
                    _TOPIC_CACHE["regex"] = None
                    
                _TOPIC_CACHE["topics"] = parsed_topics
                _TOPIC_CACHE["last_updated"] = now
        except Exception as e:
            # Fail Open: If API is down, we just keep using the old cache.
            # Only print/log if you have a logging framework setup
            pass
            
    return _TOPIC_CACHE

def main():
    # Initialize Spark Session
    spark = SparkSession.builder.appName("SimpleStreamer").getOrCreate()

    # Define the UDF (User Defined Function)
    @F.udf(returnType="array<struct<topic_id:string, term:string>>")
    def scan_text(text):
        """
        Scans a text string identifying all matching topics.
        """
        if not text: return []
        
        # Retrieve the cached regex/lookup table (lazy loading)
        cache = get_topics_cached()
        regex = cache["regex"]
        topics = cache["topics"]
        
        if not regex: return []

        # Single Pass Scan: regex.findall returns all non-overlapping matches
        # We need a SET of present terms to check against logical groups
        found_terms = set(term.lower() for term in regex.findall(text))
        
        if not found_terms: return []

        matches = []
        
        # Logic Check: "AND of ORs" (Conjunctive Normal Form - CNF)
        # For a topic to match, ALL of its rule groups (AND) must be satisfied.
        # A rule group is satisfied if AT LEAST ONE of its terms (OR) is present in the text.
        for topic in topics:
            is_match = True
            matched_term_blob = []
            
            for group_terms in topic["groups"]:
                # Check Intersection: Does the set of found_terms overlap with this group's terms?
                # This implements the "OR" logic within the group.
                # If group_terms = ["rust", "rs"], we match if we found "rust" OR "rs".
                group_matches = [t for t in group_terms if t in found_terms]
                
                if not group_matches:
                    # If any single group fails to match, the whole topic fails (AND logic).
                    is_match = False
                    break
                
                # Store evidence of the match for this group
                matched_term_blob.append(group_matches[0])
            
            if is_match:
                # Return one record per matched topic. 
                # 'term' is less relevant in CNF, passing the first matched term as representative
                matches.append((topic["id"], matched_term_blob[0]))
                    
        return matches
    
    # READ KAFKA
    raw_stream = (
        spark.readStream.format("kafka")
        .option("kafka.bootstrap.servers", KAFKA_URL)
        .option("subscribe", "reddit.raw.posts,reddit.raw.comments")
        .option("startingOffsets", "earliest")
        .load()
        .select(F.from_json(F.col("value").cast("string"), POST_SCHEMA).alias("data"))
        .select("data.*")
    )

    # TRANSFORM & AGGREGATE
    metrics_stream = (
        raw_stream
        .withColumn("matches", scan_text(F.col("text")))
        .filter("size(matches) > 0")
        .selectExpr("*", "explode(matches) as m")
        .selectExpr("*", "m.topic_id", "m.term")
        .drop("matches", "m")
        .withColumn("time", F.col("created_utc").cast("timestamp"))
        .withWatermark("time", "35 days")
        .dropDuplicates(["event_id", "topic_id"])
        .groupBy("topic_id", F.window("time", "1 day").alias("day"))
        .agg(
            F.count("*").alias("mentions"),
            F.sum(F.col("score") + F.coalesce(F.col("num_comments"), F.lit(0))).alias("engagement")
        )
        .selectExpr(
            "topic_id", "mentions", "engagement",
            "day.start as start", "day.end as end",
            "'1d' as window_type"
        )
    )

    # WRITE TO KAFKA
    (metrics_stream.select(F.to_json(F.struct("*")).alias("value"))
        .writeStream.format("kafka")
        .option("kafka.bootstrap.servers", KAFKA_URL)
        .option("topic", DAILY_METRICS_OUT)
        .option("checkpointLocation", f"{CHECKPOINT_DIR}/metrics_simple_v3")
        .outputMode("update")
        .start()
        .awaitTermination())

if __name__ == "__main__":
    main()
