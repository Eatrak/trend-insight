input {
  kafka {
    bootstrap_servers => "kafka:9092"
    topics => [
      "topic.enriched.matched.posts"
    ]
    codec => json
    # Group ID ensures consumer offset tracking. If Logstash restarts, it resumes from where it left off.
    group_id => "forward-enriched-matched-posts"
    # 'earliest' ensures that if this is a new consumer group, it processes all existing data from the start.
    auto_offset_reset => "earliest"
  }
}

filter {
  # Parse the 'timestamp' field (generated by Spark from 'created_utc') into the official @timestamp field.
  date {
    match => [ "timestamp", "ISO8601" ]
    target => "@timestamp"
  }
}

output {
  # Granular Posts (Upsert Mode)
  # Upsert ensures that if we re-process data, we don't create duplicates.
  elasticsearch {
    hosts => ["http://trend-elasticsearch:9200"]
    index => "topic-enriched-matched-posts"
    document_id => "%{topic_id}_%{event_id}"
    action => "update"
    doc_as_upsert => true
  }
}
